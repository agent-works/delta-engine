# Delta Engine Architecture Design v1.6: Context Composition Layer

**Version**: 1.6.0
**Date**: October 7, 2025
**Status**: Design Specification (Draft)
**Author**: Delta Engine Team

> **Context**: This design introduces a flexible context management system to address the fundamental bottleneck of LLM context window management. It enables memory folding, workspace guidance, and dynamic context composition through a simple, declarative protocol.

---

## 1. Overview

v1.6 introduces a **Context Composition Layer** that transforms context construction from an implicit, hardcoded process into an explicit, composable protocol. This enables sophisticated context management strategies—such as memory folding, knowledge injection, and dynamic summarization—without adding complexity to the engine core.

**Key Principles:**
- **Explicit over implicit**: Context construction is now a declarative configuration
- **Context as data**: All context components are file-based artifacts
- **Composition over built-in**: Complex context strategies are implemented as external tools
- **Default convention + optional override**: Zero-config for basic use, unlimited flexibility for advanced scenarios

**Design Philosophy:**
- From "hidden assumptions" to "visible contracts"
- From "monolithic context" to "composable blocks"
- From "engine knows how" to "user declares what"

---

## 2. Background: The Context Management Challenge

### 2.1 The Core Bottleneck

Context window management is not an optimization problem—it's a **feasibility problem**. Without proper context management, agents hit fundamental limits:

```
Real-world scenarios:
- Code review agent: 3000 lines of code + 100 git commits
- Documentation agent: 50 API files + usage examples
- Debug agent: 10 log files + Stack Overflow results

Current approach (raw journal):
→ Runs out of context by iteration 8
→ Loses critical information
→ Enters "amnesia loops"
```

As one agent developer put it: **"Context management is more critical than tools. It's the difference between toy demos and production agents."**

### 2.2 Current Limitations (v1.5)

Delta Engine v1.5 builds LLM context directly from `journal.jsonl`:

```typescript
// Simplified v1.5 approach
async rebuildConversationFromJournal(): Promise<Message[]> {
  const events = await this.journal.readJournal();

  return [
    { role: 'system', content: this.systemPrompt },
    ...this.convertEventsToMessages(events)
  ];
}
```

**Problems:**
1. **Inflexible**: No way to inject static knowledge (workspace guides, documentation)
2. **No compression**: Journal grows linearly, consuming entire context window
3. **Opaque**: Context construction strategy is buried in engine code
4. **Not composable**: Can't reuse context strategies across agents

### 2.3 Why This Matters: Lessons from Claude Code

Claude Code (the agent you're talking to) demonstrates this problem daily:
- Context window is continuously managed and compressed
- Old conversations are summarized to free up space
- Relevant files are dynamically injected based on task
- **Without context management, multi-file operations would be impossible**

Delta Engine v1.6 brings this capability to all agents.

---

## 3. Design Philosophy

### 3.1 From Implicit to Explicit

**Before (v1.5):**
```
Engine → Read journal.jsonl → Build context → Call LLM
         ↑ (Hidden, hardcoded)
```

**After (v1.6):**
```
Engine → Read context.yaml → Execute sources → Build context → Call LLM
         ↑ (Visible, declarative)
```

The act of context construction becomes **observable, debuggable, and controllable**.

### 3.2 Context as Data (Environment as Interface)

All context components are **file-based artifacts** in the workspace:
- System prompts: `system_prompt.md`
- Workspace guides: `DELTA.md`
- Compressed memories: `.delta/context_artifacts/summary.md`
- Recent conversations: `journal.jsonl`

The engine's job is to **collect and assemble**, not to **understand or interpret**.

### 3.3 Composition Over Built-in

Memory folding, knowledge graphs, vector retrieval—these are **not engine features**. They are **external tools** that produce context artifacts.

```yaml
# Memory folding is just another command
sources:
  - type: computed_file
    generator:
      command: ["python3", "tools/summarize.py"]
    output_path: "${CWD}/.delta/context_artifacts/summary.md"
```

This keeps the engine pure while enabling unlimited innovation in context engineering.

---

## 4. Core Concepts

### 4.1 The `context.yaml` Protocol

`context.yaml` is a **declarative manifest** that defines how to construct the LLM context. It lives in the agent project directory alongside `config.yaml`.

**Conceptual Model:**
```
context.yaml = Recipe for building context
sources[]    = Ordered list of context blocks (ingredients)
Engine       = Chef that follows the recipe
```

**Location:**
```
/path/to/MyAgent/
├── config.yaml          # Tools, LLM config, hooks
├── context.yaml         # ⭐ Context construction strategy (NEW)
├── system_prompt.md     # Agent instructions
└── tools/               # Custom scripts
```

### 4.2 Source Types

Three fundamental types of context sources:

#### Type 1: `file` - Static Content
Direct file injection. The simplest building block.

```yaml
- type: file
  path: "${AGENT_HOME}/system_prompt.md"
```

#### Type 2: `computed_file` - Dynamic Content
**The power move.** Execute an external command to generate context.

```yaml
- type: computed_file
  generator:
    command: ["python3", "tools/summarize.py"]
  output_path: "${CWD}/.delta/context_artifacts/summary.md"
```

This is where "memory folding" happens. The engine doesn't know how—it just calls a tool and reads the result.

#### Type 3: `journal` - Conversation History

**The original conversation rebuilding logic**, now configurable.

```yaml
- type: journal
  max_iterations: 15  # Last 15 reasoning cycles (optional)
```

> **⚠️ IMPORTANT - Different Return Type**: Unlike `file` and `computed_file` sources which return string content wrapped in system messages, journal source returns **raw `ChatCompletionMessageParam[]`** (assistant/tool messages) directly without wrapping. This preserves the native OpenAI conversation format.

**Key design insight**: Journal source is **not a "summary"**—it provides the raw OpenAI-format conversation messages (assistant/tool roles), exactly as the engine has always done. The difference is now users can **optionally limit** it to recent N iterations for token optimization.

**Behavior**:
- Without `max_iterations`: Includes **all** conversation history (v1.5 behavior)
- With `max_iterations: N`: Includes only the last N reasoning cycles

**Return format**: `ChatCompletionMessageParam[]` (native OpenAI format)
```typescript
[
  {role: 'assistant', tool_calls: [...]},
  {role: 'tool', content: '...', tool_call_id: '...'},
  // ... more assistant/tool messages
]
```

**Not a system message**: Unlike `file` and `computed_file` sources, journal returns raw conversation messages, not wrapped content.

### 4.3 Default Context Strategy (Zero-Config)

**Critical design decision**: v1.6 is **backward compatible**. Agents without `context.yaml` continue to work exactly as before.

**Default behavior (no context.yaml):**
```typescript
const DEFAULT_MANIFEST: ContextManifest = {
  sources: [
    { type: 'file', path: '${AGENT_HOME}/system_prompt.md' },
    { type: 'file', path: '${CWD}/DELTA.md', on_missing: 'skip' },
    { type: 'journal', id: 'conversation_history' }, // Unlimited history by default
  ]
};
```

**Key characteristics**:
1. Every agent gets **workspace guide support** (`DELTA.md`) for free
2. **Zero-config = full conversation history** (no token limits applied)
3. Journal source is always present (either user-configured or default)
4. Creating `context.yaml` allows explicit control over conversation history via `max_iterations`

**Design rationale**:
- Don't make assumptions about user's context window needs
- Provide mechanism (`journal` source + `max_iterations`), not policy
- Avoid "smart defaults" that may break long-running tasks
- Always include journal source to maintain backward compatibility

---

## 5. API Specification

### 5.1 `context.yaml` Schema (v1.6.0)

```yaml
# context.yaml - Root configuration file
sources:
  - type: file | computed_file | journal
    # ... type-specific fields (see below for details)
```

**Global fields (v1.6.0):**
- `sources` (required): Array of source definitions

**Deferred to v1.6.1+:**
- `total_max_tokens`: Global token budget
- `cache_policy`: Computed file caching strategy

### 5.2 Source Type Details

#### `file` Source

```yaml
- type: file
  id: workspace_guide               # Optional. Used in debug logs
  path: "${CWD}/DELTA.md"           # Required. Supports ${AGENT_HOME}, ${CWD}
  on_missing: skip                  # Optional. Default: error
                                    # skip = ignore if file not found
                                    # error = throw error if file not found
```

**Path Variables:**
- `${AGENT_HOME}`: Absolute path to agent project directory
- `${CWD}`: Absolute path to current working directory (workspace)

#### `computed_file` Source

```yaml
- type: computed_file
  id: knowledge_summary             # Optional
  generator:
    command: ["python3", "${AGENT_HOME}/tools/summarize.py"]  # Required
    timeout_ms: 30000               # Optional. Default: 30000
  output_path: "${CWD}/.delta/context_artifacts/summary.md"  # Required
  on_missing: error                 # Optional. Default: error
```

**Execution Protocol:**
1. Engine spawns `generator.command` with CWD = workspace root
2. Command writes output to `output_path` (generator is responsible for creating output directory if needed)
3. Engine reads `output_path` and injects content into context

**Environment variables passed to generator:**
- `DELTA_RUN_ID`: Current run ID
- `DELTA_AGENT_HOME`: Agent project directory
- `DELTA_CWD`: Workspace directory

**Note:** The generator command must create the output directory structure (e.g., `.delta/context_artifacts/`) if it doesn't exist. The engine only reads the output file and does not create directories.

#### `journal` Source

```yaml
- type: journal
  id: recent_conversation           # Optional
  max_iterations: 15                # Optional. Omit for all conversation
```

**Behavior:**
- Reads `journal.jsonl` from the current run
- Reconstructs conversation as `ChatCompletionMessageParam[]` (assistant/tool messages)
- `max_iterations: N` limits to the most recent N reasoning cycles
- Without `max_iterations`: Includes **all** conversation (v1.5 behavior)

**Return type**: Unlike other sources, journal returns an **array of messages** (not wrapped string):
```typescript
// file/computed_file return wrapped system message:
{role: 'system', content: '# Context Block: id\n\n...'}

// journal returns native OpenAI conversation:
[
  {role: 'assistant', content: null, tool_calls: [...]},
  {role: 'tool', content: '...', tool_call_id: '...'},
]
```

**Terminology note**: We use `max_iterations` (not `max_turns`) to align with engine terminology (engine.ts iteration counter, CLI `--max-iterations`). One iteration = one complete Think-Act-Observe cycle (1 THOUGHT + tool calls + results).

**Deferred to v1.6.1+:**
- `strategy: truncate_head`: Drop oldest messages if exceeds budget
- `max_tokens`: Token limit for journal content

### 5.3 Complete Example

```yaml
# context.yaml for a code review agent
sources:
  # Priority 1: Core instructions
  - type: file
    id: system_prompt
    path: "${AGENT_HOME}/system_prompt.md"

  # Priority 2: Workspace context (auto-skip if missing)
  - type: file
    id: workspace_guide
    path: "${CWD}/DELTA.md"
    on_missing: skip

  # Priority 3: Compressed knowledge (dynamic)
  - type: computed_file
    id: codebase_summary
    generator:
      command: ["tree-sitter-summarize", "--repo", "${CWD}"]
    output_path: "${CWD}/.delta/context_artifacts/codebase.md"

  # Priority 4: Recent conversation (fills remaining space)
  - type: journal
    id: recent_dialogue
    max_iterations: 20
```

**Key insight:** Source order = priority order. Top sources get included first.

---

## 6. Implementation Architecture

### 6.1 ContextBuilder Workflow

```typescript
// High-level architecture
class ContextBuilder {
  async build(): Promise<Message[]> {
    // 1. Load manifest (or use default)
    const manifest = await this.loadManifest();

    // 2. Process sources in order
    const messages: Message[] = [];
    for (const source of manifest.sources) {
      const content = await this.processSource(source);
      if (content) {
        messages.push({ role: 'system', content });
      }
    }

    return messages;
  }

  private async processSource(source: ContextSource): Promise<string | null> {
    switch (source.type) {
      case 'file':
        return this.processFileSource(source);
      case 'computed_file':
        return this.processComputedFile(source);
      case 'journal':
        return this.processJournalSource(source);
    }
  }
}
```

**Processing pipeline:**
```
Load manifest → For each source → Process → Collect content → Assemble messages
```

### 6.2 Engine Integration

```typescript
// src/engine.ts (modified)
private async rebuildConversationFromJournal(): Promise<Message[]> {
  const builder = new ContextBuilder(
    this.ctx.agentHome,
    this.ctx.cwd,
    this.journal
  );

  return await builder.build();
}
```

**Key design decision:** All context construction logic is **extracted from engine.ts** into a dedicated `src/context/` module. This keeps the engine core clean and makes context logic testable.

### 6.3 File Structure

```
src/
└── context/                      # ⭐ New module
    ├── types.ts                  # Zod schemas for context.yaml
    ├── builder.ts                # ContextBuilder class
    ├── sources/
    │   ├── file-source.ts        # FileSource processor
    │   ├── computed-source.ts    # ComputedFileSource processor
    │   └── journal-source.ts     # JournalSource processor
    └── index.ts                  # Public API exports
```

**Design principle:** One file per source type. Easy to extend with new source types in future.

---

## 7. Use Cases and Examples

### 7.1 Workspace Guide (DELTA.md)

**Scenario:** Agent enters a complex codebase. How does it understand project conventions?

**Solution:** Create `DELTA.md` in the workspace:
```markdown
# Project Guide

## Architecture
This is a microservices project. Key services:
- auth-service: Port 3001
- api-gateway: Port 3000

## Running Tests
Use `npm test` in each service directory.

## Common Pitfalls
- Database migrations must be run manually
- Redis must be running for auth-service
```

**Result:** Every LLM invocation includes this guide. Agent "knows" the workspace.

**No configuration needed.** Default `context.yaml` includes:
```yaml
- type: file
  path: "${CWD}/DELTA.md"
  on_missing: skip
```

### 7.2 Memory Folding (Compression)

**Scenario:** Agent has 50 iterations of conversation. Context window is full.

**Solution:** Create a summarization tool:
```python
# tools/summarize.py
import sys
import json

# Read journal
journal = read_journal('.delta/LATEST/journal.jsonl')

# Extract key facts
facts = extract_key_decisions(journal)

# Write compressed summary
with open('.delta/context_artifacts/summary.md', 'w') as f:
    f.write(format_summary(facts))
```

**Configure context.yaml:**
```yaml
sources:
  - type: file
    path: "${AGENT_HOME}/system_prompt.md"

  - type: computed_file
    generator:
      command: ["python3", "${AGENT_HOME}/tools/summarize.py"]
    output_path: "${CWD}/.delta/context_artifacts/summary.md"

  - type: journal
    max_iterations: 10  # Only recent conversation
```

**Result:** Context = instructions + compressed history + recent 10 turns.

**Extensibility:** Replace Python script with:
- Vector retrieval (find relevant docs)
- Knowledge graph (extract entities)
- LLM-based summarization (Claude Haiku for compression)

### 7.3 Dynamic Context Adaptation

**Scenario:** Code review agent needs different context depending on file type.

**Solution:** Create a smart context generator:
```bash
# tools/context-for-file.sh
#!/bin/bash
FILE_TYPE=$(echo "$1" | awk -F. '{print $NF}')

case $FILE_TYPE in
  py)
    cat docs/python-style-guide.md
    ;;
  ts)
    cat docs/typescript-conventions.md
    ;;
  *)
    echo "No specific guidelines"
    ;;
esac
```

**Configure context.yaml:**
```yaml
sources:
  - type: computed_file
    generator:
      command: ["${AGENT_HOME}/tools/context-for-file.sh", "${TARGET_FILE}"]
    output_path: "${CWD}/.delta/context_artifacts/language-guide.md"
```

**Note:** `${TARGET_FILE}` would need to be injected via environment variable by the orchestrating agent.

### 7.4 Team Collaboration (Context Handoff)

**Scenario:** Parent agent calls sub-agent. How does it provide context?

**Solution:** Parent agent creates a custom `context.yaml` for the sub-agent:
```typescript
// Parent agent's orchestration logic
const subAgentContext = {
  sources: [
    { type: 'file', path: '${AGENT_HOME}/system_prompt.md' },
    { type: 'file', path: '${CWD}/mission-brief.md' },  // Parent writes this
    { type: 'file', path: '${CWD}/../parent-summary.md' }  // Parent's state
  ]
};

fs.writeFileSync(
  'sub-workspace/context.yaml',
  yaml.stringify(subAgentContext)
);
```

**Result:** Sub-agent receives a **curated information package** from the parent, not just a task string.

---

## 8. Design Decisions

### 8.1 Why `context.yaml` and not `context_manifest.yaml`?

**Decision:** Use `context.yaml`.

**Rationale:**
- **Brevity**: 4 characters vs 16 characters
- **Symmetry**: Pairs with `config.yaml` (config = tools, context = knowledge)
- **Clarity**: Name directly expresses purpose
- **Convention**: Matches ecosystem patterns (`tsconfig.json`, `docker-compose.yaml`)

### 8.2 Why "override" instead of "extend" default?

**Decision:** Creating `context.yaml` **replaces** the default strategy entirely.

**Rationale:**
- **Simplicity**: One code path, not two
- **Explicitness**: No hidden merging logic
- **Control**: User has complete authority over context
- **Escape hatch**: If user wants default + custom, they can replicate it:
  ```yaml
  sources:
    - type: file
      path: "${AGENT_HOME}/system_prompt.md"
    - type: file
      path: "${CWD}/DELTA.md"
      on_missing: skip
    - type: journal
      max_iterations: 30
    # ... now add custom sources
  ```

### 8.3 Why no token budget in v1.6.0?

**Decision:** Defer `total_max_tokens` and per-source `max_tokens` to v1.6.1+.

**Rationale:**
- **Complexity**: Token counting requires external libraries (`tiktoken`)
- **Model variance**: Different models use different tokenizers
- **Truncation challenges**: Cutting content can break Markdown/code structure
- **Validation needs**: Need real-world data to design budget system correctly

**Alternative:** Users can implement token management in their `computed_file` generators:
```yaml
- type: computed_file
  generator:
    command: ["summarize.py", "--max-tokens", "2000"]
  output_path: "${CWD}/.delta/context_artifacts/summary.md"
```

Tool-level token management is simpler and more flexible than engine-level.

### 8.4 Why environment variables for generator communication?

**Decision:** Pass `DELTA_RUN_ID`, `DELTA_AGENT_HOME`, etc. as environment variables.

**Rationale:**
- **Universal**: Works with any language (Python, Bash, Node.js)
- **No serialization**: Avoid complex JSON parsing
- **Standard practice**: Follows Unix conventions
- **Future-proof**: Easy to add more variables without breaking compatibility

---

## 9. Future Extensions (Out of Scope for v1.6.0)

### 9.1 Token Budget System (v1.6.1)

```yaml
total_max_tokens: 8000  # Global budget

sources:
  - type: file
    path: "${AGENT_HOME}/system_prompt.md"
    # No max_tokens = must include fully

  - type: computed_file
    max_tokens: 2000  # Hard limit
    generator:
      command: ["summarize.py", "--target-tokens", "2000"]
    output_path: "${CWD}/.delta/context_artifacts/summary.md"

  - type: journal
    # Uses remaining budget
```

**Accounting model:** Engine tracks consumed tokens, allocates remainder to flexible sources.

### 9.2 Conditional Loading (v1.6.2)

```yaml
- type: file
  path: "${CWD}/api-docs.md"
  condition:
    when: task_contains
    pattern: "API|endpoint|REST"
```

**Use case:** Load expensive context only when task requires it.

### 9.3 Caching (v1.6.2)

```yaml
- type: computed_file
  generator:
    command: ["expensive-analysis.py"]
  output_path: "${CWD}/.delta/context_artifacts/analysis.md"
  cache:
    strategy: file_hash
    invalidate_on: ["src/**/*.py"]  # Recompute if source files change
```

**Use case:** Avoid re-running expensive generators when input hasn't changed.

### 9.4 Context Pipelines (v1.7)

```yaml
- type: pipeline
  stages:
    - generator: ["retrieve-docs.py"]  # Stage 1: Retrieve
      output: "${CWD}/.delta/tmp/docs.json"
    - generator: ["rank-docs.py"]      # Stage 2: Rerank
      output: "${CWD}/.delta/tmp/ranked.json"
    - generator: ["format-docs.py"]    # Stage 3: Format
      output: "${CWD}/.delta/context_artifacts/final.md"
```

**Use case:** Multi-stage context refinement (retrieve → rerank → format).

### 9.5 Context Package Manager (v2.0)

```bash
npm install @delta/context-react-dev
```

**Result:** Pre-configured context strategies published as npm packages.

```yaml
# Import published strategy
import: "@delta/context-react-dev"

# Override specific sources
sources:
  - type: computed_file
    generator:
      command: ["custom-summarizer.py"]
```

**Use case:** Share and reuse context engineering best practices.

---

## 10. Summary

### 10.1 What v1.6 Delivers

**Core capability:**
- Declarative context composition via `context.yaml`
- Three source types: `file`, `computed_file`, `journal`
- Default strategy (system_prompt + DELTA.md + journal) for zero-config operation
- Full backward compatibility (no breaking changes)

**Architectural benefits:**
- **Separation of concerns**: Context logic extracted from engine core
- **Testability**: ContextBuilder is independently testable
- **Extensibility**: New source types can be added without modifying engine
- **Debuggability**: Context construction is visible and traceable

**Philosophical alignment:**
- ✅ **Everything is a Command**: Memory folding = external tool call
- ✅ **Environment as Interface**: Context sources are file-based
- ✅ **Composition over Built-in**: No context logic in engine core

### 10.2 What v1.6 Enables

**For agent developers:**
- Implement memory folding without modifying engine code
- Create workspace-specific guides (`DELTA.md`) that agents auto-load
- Dynamically inject knowledge (vector retrieval, knowledge graphs)
- Control context construction with surgical precision

**For researchers:**
- Experiment with context strategies as configuration, not code
- Share and compare context engineering approaches
- Build context optimization tools as standalone utilities
- Validate hypotheses about attention management

**For the ecosystem:**
- Foundation for a "context package manager"
- Standard protocol for context handoff between agents
- Clear path to advanced features (token budgets, caching, pipelines)

### 10.3 Key Insight

The v1.6 Context Composition Layer doesn't solve context management—it **creates the platform** for solving it. By providing a simple, composable protocol, we enable the community to innovate on context engineering while keeping the Delta Engine core pristine.

**Context engineering is now a first-class discipline**, not a buried implementation detail.

---

**Status:** Ready for implementation.
**Next Steps:** See `docs/implementation/v1.6-context-layer.md` for detailed implementation plan.
